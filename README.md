# üìä An√°lise de Dados com PySpark

## üìå Sobre o Projeto

Este reposit√≥rio re√∫ne estudos e pr√°ticas utilizando **PySpark** para processamento e an√°lise de dados em ambiente distribu√≠do.

O objetivo √© aplicar conceitos fundamentais de Big Data, como leitura, transforma√ß√£o, agrega√ß√£o e escrita de dados, utilizando a API de DataFrames do Spark.

Os experimentos foram desenvolvidos no **Google Colab**, simulando cen√°rios reais de manipula√ß√£o de m√∫ltiplas fontes de dados.

## üß™ Notebook 1 ‚Äî Leitura e Escrita de Dados (`leitura_escrita.ipynb`)

Este notebook apresenta os fundamentos do uso do PySpark para:

- Cria√ß√£o da `SparkSession`
- Leitura de arquivos CSV
- Explora√ß√£o inicial dos dados com DataFrames
- Visualiza√ß√£o de colunas e estrutura
- Escrita de dados processados

Foram utilizados dois conjuntos de dados:

- `videos-stats.csv`
- `comments.csv`

Ambos dispon√≠veis na pasta `data/`.

Este notebook estabelece a base para os pr√≥ximos estudos e an√°lises que ser√£o adicionados ao reposit√≥rio.

## üß™ Notebook 2 ‚Äî Tratamento e Transforma√ß√£o de Dados (`tratamento.ipynb`)

Neste notebook s√£o aplicadas opera√ß√µes de transforma√ß√£o utilizando a API de DataFrames do PySpark, aprofundando o processamento dos dados carregados anteriormente.

Principais etapas desenvolvidas:

- Sele√ß√£o de colunas espec√≠ficas
- Filtros condicionais
- Manipula√ß√£o e cria√ß√£o de colunas
- Convers√£o e tratamento de tipos de dados
- Limpeza e organiza√ß√£o das informa√ß√µes

Este notebook demonstra a etapa de prepara√ß√£o dos dados, essencial em projetos de an√°lise e engenharia de dados, antes da realiza√ß√£o de agrega√ß√µes ou an√°lises mais complexas.

## üß™ Notebook 3 ‚Äî Prepara√ß√£o dos Dados (`preparacao.ipynb`)

Este notebook aprofunda a etapa de prepara√ß√£o dos dados, consolidando transforma√ß√µes realizadas anteriormente e organizando as informa√ß√µes para futuras an√°lises.

Entre as atividades desenvolvidas est√£o:

- Ajustes finais na estrutura dos DataFrames
- Tratamento complementar de colunas
- Organiza√ß√£o e padroniza√ß√£o dos dados
- Prepara√ß√£o para an√°lises e agrega√ß√µes posteriores

Essa etapa √© fundamental em projetos de dados, pois garante consist√™ncia, qualidade e confiabilidade antes da gera√ß√£o de insights.

## üß™ Notebook 4 ‚Äî Agrega√ß√µes e An√°lises (`agregacao.ipynb`)

Neste notebook s√£o aplicadas opera√ß√µes de agrega√ß√£o utilizando o m√©todo `groupBy()` em conjunto com fun√ß√µes anal√≠ticas do PySpark.

Principais opera√ß√µes desenvolvidas:

- Agrupamento de dados por colunas espec√≠ficas
- C√°lculo de m√©tricas como:
  - Contagem (`count`)
  - Soma (`sum`)
  - M√©dia (`avg`)
- Organiza√ß√£o dos resultados para an√°lise

Essa etapa representa a transi√ß√£o do processamento de dados para a gera√ß√£o de informa√ß√µes consolidadas, permitindo identificar padr√µes e m√©tricas relevantes a partir dos datasets.
